
[[./img/latplanlogo-simple.svg.png]]

* LatPlan : A domain-independent, image-based classical planner

*Use tagged versions for reliable reproduction of the results.*

+ *NEWS* Initial release: Published after AAAI18.
+ *NEWS* Updates on version 2: Mainly the refactoring. AAAI18 experiments still works.
+ *NEWS* Updates on version 2.1: Version 2 was not really working and I could finally have time to fix it. Added ZSAE experiments.
+ *NEWS* Updates on version 2.2: Backported more minor improvements from the private repository
+ *NEWS* Updates on version 3.0: Updates for DSAMA system. More refactoring is done on the learner code.
  We newly introduced ama3-planner which takes a PDDL domain file.
+ *NEWS* Updates on version 4.0: Updates for Cube-Space AE in IJCAI2020 paper.
  + This is a version that can exploit the full potential of search heuristics (lower bounds in Branch-and-Bound),
    such as LM-cut or Bisimulation Merge-and-Shrink in Fast Downward.
  + We added the tuned hyperparameters to the repository so that reproducing the experiments will be easy(ish).
  + We now included a script for 15-puzzle instances randomly sampled from the states
    14 or 21 steps away from the goal.
  + We also included a script for Korf's 100 instances of 15-puzzle, but the accuracy
    was not sufficient in those problems where the shortest path length are typically around 50.
    Problems that require deeper searches also require more model accuracy because
    errors accumulate in each state transition.
+ *NEWS* Updates on version 4.1:
  + Improved the installation procedure. Now I recommend =conda= based installation, which specifies the correct Keras + TF versions.
  + The repository is now reorganized so that the library code goes to =latplan= directory and all other scripts remain in the root directory.
    During this migration I used [[https://github.com/newren/git-filter-repo][git-filter-repo]] utility, which rewrites the history.
    This may have broken the older tags --- I will inspect the breakage and fix them soon.
+ *NEWS* Updates on version 4.1.1, 4.1.2: The project is loadable with Anaconda, is pip-installable (somewhat).
+ *NEWS* Updates on version 4.1.3: Minor refactoring. We also released the trained weights. See [[https://github.com/guicho271828/latplan/releases][Releases]].
+ *NEWS* Updates on version 5: Updates for Bidirectional Cube-Space AE in JAIR paper.
  + AMA3+ Cube-Space AE : A revised version of AMA3 Cube-Space AE (IJCAI20 version) which is now modeled as a sound generative model.
  + AMA4+ Bidirectional Cube-Space AE : An extension of Cube-Space AE which can learn both effects and preconditions.
    
# [[https://travis-ci.org/guicho271828/latplan][https://travis-ci.org/guicho271828/latplan.svg?branch=master]]

This repository contains the source code of LatPlan.

+ Asai, Kajino, Fukunaga, Muise: 2021. Classical Planning in Deep Latent Space.
  + Under review in JAIR  https://arxiv.org/abs/XXX
+ Asai, M; Muise, C.: 2020. Learning Neural-Symbolic Descriptive Planning Models via Cube-Space Priors: The Voyage Home (to STRIPS).
  + *Accepted* in IJCAI-2020 (Accept ratio 12.6%). https://arxiv.org/abs/2004.12850
+ Asai, M.: 2019. Neural-Symbolic Descriptive Action Model from Images: The Search for STRIPS.
  + https://arxiv.org/abs/1912.05492
+ Asai, M.: 2019. Unsupervised Grounding of Plannable First-Order Logic Representation from Images (code available from https://github.com/guicho271828/latplan-fosae)
  + *Accepted* in ICAPS-2019, Learning and Planning Track. https://arxiv.org/abs/1902.08093
+ Asai, M.; Kajino, F: 2019. Towards Stable Symbol Grounding with Zero-Suppressed State AutoEncoder
  + *Accepted* in ICAPS-2019, Learning and Planning Track. https://arxiv.org/abs/1903.11277
+ Asai, M.; Fukunaga, A: 2018. Classical Planning in Deep Latent Space: Breaking the Subsymbolic-Symbolic Boundary.
  + *Accepted* in AAAI-2018. https://arxiv.org/abs/1705.00154
+ Asai, M.; Fukunaga, A: 2017. Classical Planning in Deep Latent Space: From Unlabeled Images to PDDL (and back).
  + In /Knowledge Engineering for Planning and Scheduling (KEPS) Workshop (ICAPS2017)/.
  + In Cognitum Workshop at ICJAI-2017.
  + In Neural-Symbolic Workshop 2017.

* Setup

On Ubuntu, prerequisites can be installed via launching [[./install.sh]] (It requires =sudo= several times).
OSX users should be able to find the equivalents in homebrew. We listed the requirements.

** General Requirements

Python 3.5 or later is required.

+ =mercurial g++ cmake make python flex bison g++-multilib= --- these are required for compiling Fast Downward.

+ =git build-essential automake libcurl4-openssl-dev= --- these are required for compiling [Roswell](http://roswell.github.io/).
 OSX users should use =brew install roswell=.

+ =gnuplot= --- for plotting.

+ =parallel= --- for running some scripts.

+ =libmagic-dev= --- for filetype detection used by file processor.

+ Common Lisp library dependencies (lines started by =ros= in  [[./install.sh]] )
  + =ros dynamic-space-size=8000 install numcl arrival eazy-gnuplot=
  + =ros dynamic-space-size=8000 install guicho271828/magicffi guicho271828/dataloader=

** Python Dependency Installation with Anaconda / Miniconda (recommended)

=anaconda= / =miniconda= (https://docs.conda.io/en/latest/miniconda.html) is a
python version dependency & mini environment management system.
We recommend using =miniconda=, as it is smaller.

Run =conda env create -f environment.yml= then =conda activate latplan=.

Also run =./setup.py install=, which install =latplan=.

** Python Dependency Installation without Anaconda / Miniconda on Ubuntu

You should install =python3-pip= and =python3-pil= from the APT repository.
Afterwards, run =./setup.py install=, which installs =latplan=.


* Command Line Interface

Installing the latest version of Latplan via =pip= creates a runnable =latplan= script in =~/.local/bin=.
The script is not usable for running the experiments (see the next section) because it has an empty hyperparameter.
However, it has the same command line API as =train_common.py=, =train_kltune.py=, and so on,
therefore it may be useful for you to understand the command line API for those scripts.

#+begin_verbatim
(latplan) 07/05 08:08 latplan$ latplan -h
WARNING:tensorflow:Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them.
WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.
Using TensorFlow backend.
Default float: float32
usage: latplan [-h] mode subcommand ...

positional arguments:
  mode          A string which contains mode substrings.
                Recognized modes are:
                
                   learn     : perform the training with a hyperparameter tuner. Results are stored in logs/.
                   plot      : load the weights of the current best hyperparameter and produce visualizations
                   dump      : dump csv files necessary for producing PDDL models
                   summary   : perform extensive evaluations and collect the statistics, store the result in performance.json
                   debug     : debug training limited to epoch=2, batch_size=100. dataset is truncated to 200 samples
                   reproduce : train the best hyperparameter so far three times with different random seeds. store the best results.
                   iterate   : iterate plot/dump/summary commands above over all hyperparmeters that are already trained and stored in logs/ directory.
                
                For example, learn_plot_dump contains 'learn', 'plot', 'dump' mode.
                The separater does not matter because its presense is tested by python's `in` directive, i.e., `if 'learn' in mode:` .
                Therefore, learnplotdump also works.

optional arguments:
  -h, --help    show this help message and exit

subcommand:
  
  A string which matches the name of one of the dataset functions in latplan.main module.
  
  Each task has a different set of parameters, e.g.,
  'puzzle' has 'type', 'width', 'height' where 'type' should be one of 'mnist', 'spider', 'mandrill', 'lenna',
  while 'lightsout' has 'type' being either 'digital' and 'twisted', and 'size' being an integer.
  See subcommand help.

  subcommand
    hanoi       Tower of Hanoi.
    puzzle      Sliding tile puzzle.
    puzzle_objs
                Object-based sliding tile puzzle.
    lightsout   LightsOut game (see https://en.wikipedia.org/wiki/Lights_Out_(game))
    sokoban     Sokoban environment rendered by PDDLGym.
    sokoban_objs
                Object-based Sokoban environment rendered by PDDLGym.
    blocks      Blocksworld environment. Requires an archive made by github.com/IBM/photorealistic-blocksworld
    blocks_objs
                Object-based blocksworld environment. Requires an archive made by github.com/IBM/photorealistic-blocksworld
#+end_verbatim

#+begin_verbatim
(latplan) 07/05 08:09 latplan$ latplan learn hanoi -h
WARNING:tensorflow:Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them.
WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.
Using TensorFlow backend.
Default float: float32
usage: latplan mode hanoi [-h] disks towers num_examples aeclass [comment]

positional arguments:
  disks         The number of disks in the environment.
  towers        The number of towers, or the width of the environment.
  num_examples  Number of data points to use. 90% of this number is used for training, and 5% each for validation and testing. It is
                assumed that the user has already generated a dataset archive in latplan/puzzles/, which contains a larger number of
                data points using the setup-dataset script provided in the root of the repository.
  aeclass       A string which matches the name of the model class available in latplan.model module. It must be one of: AE StateAE
                ZeroSuppressStateAE VanillaTransitionAE HammingTransitionAE CosineTransitionAE PoissonTransitionAE
                ConcreteDetConditionalEffectTransitionAE ConcreteDetBoolMinMaxEffectTransitionAE
                ConcreteDetBoolSmoothMinMaxEffectTransitionAE ConcreteDetLogitAddEffectTransitionAE
                ConcreteDetLogitAddEffect2TransitionAE ConcreteDetNormalizedLogitAddEffectTransitionAE CubeSpaceAE_AMA3
                ConcreteDetNormalizedLogitAddBidirectionalTransitionAE CubeSpaceAE_AMA4 ConcreteDetLogitAddEffectTransitionAEPlus
                ConcreteDetLogitAddEffect2TransitionAEPlus ConcreteDetNormalizedLogitAddEffectTransitionAEPlus
                ConvolutionalConcreteDetNormalizedLogitAddEffectTransitionAEPlus CubeSpaceAE_AMA3Plus CubeSpaceAE_AMA3Conv
                ConcreteDetNormalizedLogitAddBidirectionalTransitionAEPlus
                ConvolutionalConcreteDetNormalizedLogitAddBidirectionalTransitionAEPlus CubeSpaceAE_AMA4Plus CubeSpaceAE_AMA4Conv
  comment       A string which is appended to the directory name to label each experiment. (default: )

optional arguments:
  -h, --help    show this help message and exit
#+end_verbatim


* Running

Next, customize the following files for your job scheduler before running.
The job submission commands are stored in a variable =$common=, which by default
has the value like =jbsub -mem 32g -cores 1+1 -queue x86_24h=, which means
the jobs are submitted to a 24 hour runtime limit queue, requesting 1 cpu, 1 gpu (1+1) and 32g memory.
You also need to uncomment the commands to run.
By default, everything is commented out and nothing runs.

#+begin_src sh
# You first need to set up a dataset.
./setup-dataset.sh

# Submit the jobs for training AMA3+ (Cube-Space AEs) and AMA4+ (Bidirectional Cube-Space AEs)
./train_propositional.sh

# Submit the jobs for converting the training results into PDDL files
./pddl-ama3.sh

# Generates problem instances.
problem-generators/generate-all

# Copies the generated instances into a target directory.
problem-generators/copy propositional [directory]

# Edit run_ama3_all.sh to specify appropriate target directory and then submit the jobs for planning.
# To reproduce the exact same experiments in the paper,
# approximately 400 jobs are submitted. Each job requires 8 cores, no GPUs, and takes 6 hours maximum.
# Details can be customized for your compute environment.
./run_ama3_all.sh 

# After the experiments, run this to generate the tables and figures.
# for details read the source code.
make -C tables

#+end_src

** file structure

+ Library code
  + latplan/main/*.py :: Each file contains source code for loading the dataset and launching the training.
  + latplan/model.py :: network definitions.
  + latplan/mixins/*.py :: Contains various mixin classes used to build a complex neural network.
  + latplan/util/ :: contains general-purpose utility functions for python code.
  + latplan/puzzles/ :: code for domain generators/validators.
    + puzzles/*.py :: each file represents a domain. 
    + puzzles/model/*.py :: the core model (successor rules etc.) of the domain. this is disentangled from the images.
+ Scripts
  + strips.py :: (Bad name!) the program for training an SAE,
                 and writes the propositional encoding of states/transitions to a CSV file.
  + ama1-planner.py :: Latplan using AMA1. (obsolete)
  + ama2-planner.py :: Latplan using AMA2. (obsolete)
  + ama3-planner.py :: Latplan using visual inputs (init, goal) and a PDDL domain file.
  + run_ama{1,2,3}_all.sh :: Run all experiments.
  + helper/ :: helper scripts for AMA1.
  + problem-generators/ :: scripts for generating problem instances.
+ tests/ :: test files, mostly the unit tests for domain generator/validator
+ samples/ :: where the learned results should go. Each SAE training results are stored in a subdirectory.
+ tables/ :: code for storing experimental results into SQLITE and generating tables and figures.
+ (git submodule) planner-scripts/ :: My personal scripts for invoking domain-independent planners.
     Not just Fast Downward.

** Gallery

[[./img/hanoi_4_3_36_81_conv_blind_path_0.png]]
[[./img/lightsout_digital_4_36_20000_conv_Astar_path_0.png]]
[[./img/lightsout_twisted_4_36_20000_conv_Astar_path_0.png]]
[[./img/puzzle_mandrill_3_3_36_20000_conv_blind_path_0.png]]
[[./img/puzzle_mnist_3_3_36_20000_conv_blind_path_0.png]]
[[./img/puzzle_spider_3_3_36_20000_conv_blind_path_0.png]]
